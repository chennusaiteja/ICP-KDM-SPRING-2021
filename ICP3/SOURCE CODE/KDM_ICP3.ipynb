{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zgV-i7SNOG"
      },
      "source": [
        "Triplet Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n2feWZPSSSm",
        "outputId": "c5af17a0-f87a-40ed-b28c-4ddd3fba69d5"
      },
      "source": [
        "pip install textacy"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textacy in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: jellyfish>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.8.2)\n",
            "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.2.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn<0.24.0,>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.11.0)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.0.5)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.2.4)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.41.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.5)\n",
            "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.10.0)\n",
            "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.11.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (53.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i1R0KvsSgse",
        "outputId": "e61f1c86-29af-4082-b647-01ddbed44a99"
      },
      "source": [
        "#!/usr/bin/env python\r\n",
        "from __future__ import unicode_literals\r\n",
        "# Load Library files\r\n",
        "import en_core_web_sm\r\n",
        "import spacy\r\n",
        "import textacy\r\n",
        "nlp = en_core_web_sm.load()\r\n",
        "SUBJ = [\"nsubj\",\"nsubjpass\"] \r\n",
        "VERB = [\"ROOT\"] \r\n",
        "OBJ = [\"dobj\", \"pobj\", \"dobj\"] \r\n",
        "text = nlp(u'The cat sat on the mat. The cat jumped and picked up the biscuit. The cat ate biscuit and cookies.')\r\n",
        "sub_toks = [tok for tok in text if (tok.dep_ in SUBJ) ]\r\n",
        "obj_toks = [tok for tok in text if (tok.dep_ in OBJ) ]\r\n",
        "vrb_toks = [tok for tok in text if (tok.dep_ in VERB) ]\r\n",
        "text_ext = list(textacy.extract.subject_verb_object_triples(text))\r\n",
        "print(\"Subjects:\", sub_toks)\r\n",
        "print(\"VERB :\", vrb_toks)\r\n",
        "print(\"OBJECT(s):\", obj_toks)\r\n",
        "print (\"SVO:\", text_ext)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Subjects: [cat, cat, cat]\n",
            "VERB : [sat, jumped, ate]\n",
            "OBJECT(s): [mat, biscuit, biscuit]\n",
            "SVO: [(cat, ate, biscuit), (cat, ate, cookies)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_6ixtH8TB71",
        "outputId": "5d7734fb-01ce-41c6-d1c7-4de21d3cc8d6"
      },
      "source": [
        "import spacy\r\n",
        "\r\n",
        "import textacy\r\n",
        "\r\n",
        "nlp = spacy.load('en')\r\n",
        "text = nlp(u'Startup companies create jobs and support innovation. Hilary supports entrepreneurship.')\r\n",
        "\r\n",
        "text_ext = textacy.extract.subject_verb_object_triples(text)\r\n",
        "list(text_ext)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(companies, create, innovation), (Hilary, supports, entrepreneurship)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6Vnz5gTK0V"
      },
      "source": [
        "WordNet Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaAxhCm-TZc1",
        "outputId": "72afb2aa-f724-493a-913c-65bc86790112"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_TygG4PTNSm",
        "outputId": "47fa4df6-cb84-4b55-f1bc-6fab948d16ff"
      },
      "source": [
        "# lets use word paint as an exqmple\r\n",
        "syns = wordnet.synsets(\"good\")\r\n",
        "\r\n",
        "# An example of a synset:\r\n",
        "print(syns[0].name())\r\n",
        "print('\\n')\r\n",
        "# Just the word:\r\n",
        "print(syns[0].lemmas()[0].name())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# Definition of that first synset:\r\n",
        "print(syns[0].definition())\r\n",
        "print('\\n')\r\n",
        "# Examples of the word in use in sentences:\r\n",
        "print(syns[0].examples())\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print('Set of hyponyms:\\n', syns[0].hyponyms(), '\\n' )\r\n",
        "print('Set of hypernyms:\\n', syns[0].hypernyms(), '\\n' )\r\n",
        "print('Set of root-hypernyms:\\n', wordnet.synset('dog.n.01').root_hypernyms(), '\\n' )\r\n",
        "print('Set of common-hypernyms:\\n', wordnet.synset('dog.n.01').common_hypernyms(wordnet.synset('cat.n.01')) , '\\n' )\r\n",
        "print('Set of lowest-common-hypernyms:\\n', wordnet.synset('dog.n.01').lowest_common_hypernyms(wordnet.synset('cat.n.01')) , '\\n' )\r\n",
        "print('Set of part-meronyms:\\n', wordnet.synset('table.n.2').part_meronyms(), '\\n' )\r\n",
        "print('Set of member-meronyms:\\n', wordnet.synset('faculty.n.2').member_meronyms() , '\\n' )\r\n",
        "print('Set of member-holonyms:\\n', wordnet.synset('kitchen.n.01').part_holonyms() , '\\n' )\r\n",
        "print('Set of part-holonyms:\\n', wordnet.synset('course.n.7').part_holonyms(), '\\n' )\r\n",
        "print('Set of substance-holonyms:\\n', wordnet.synset('gin.n.1').substance_holonyms()  , '\\n' )\r\n",
        "print('Set of substance-meronyms:\\n', wordnet.synset('water.n.1').substance_meronyms()  , '\\n' )\r\n",
        "print('Entailment of word Snore:\\n', wordnet.synset('snore.v.01').entailments(), '\\n' )\r\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good.n.01\n",
            "\n",
            "\n",
            "good\n",
            "\n",
            "\n",
            "benefit\n",
            "\n",
            "\n",
            "['for your own good', \"what's the good of worrying?\"]\n",
            "\n",
            "\n",
            "Set of hyponyms:\n",
            " [Synset('common_good.n.01')] \n",
            "\n",
            "Set of hypernyms:\n",
            " [Synset('advantage.n.01')] \n",
            "\n",
            "Set of root-hypernyms:\n",
            " [Synset('entity.n.01')] \n",
            "\n",
            "Set of common-hypernyms:\n",
            " [Synset('placental.n.01'), Synset('physical_entity.n.01'), Synset('living_thing.n.01'), Synset('vertebrate.n.01'), Synset('animal.n.01'), Synset('carnivore.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('mammal.n.01'), Synset('organism.n.01'), Synset('entity.n.01'), Synset('chordate.n.01')] \n",
            "\n",
            "Set of lowest-common-hypernyms:\n",
            " [Synset('carnivore.n.01')] \n",
            "\n",
            "Set of part-meronyms:\n",
            " [Synset('leg.n.03'), Synset('tabletop.n.01'), Synset('tableware.n.01')] \n",
            "\n",
            "Set of member-meronyms:\n",
            " [Synset('professor.n.01')] \n",
            "\n",
            "Set of member-holonyms:\n",
            " [Synset('dwelling.n.01')] \n",
            "\n",
            "Set of part-holonyms:\n",
            " [Synset('meal.n.01')] \n",
            "\n",
            "Set of substance-holonyms:\n",
            " [Synset('gin_and_it.n.01'), Synset('gin_and_tonic.n.01'), Synset('martini.n.01'), Synset('pink_lady.n.01')] \n",
            "\n",
            "Set of substance-meronyms:\n",
            " [Synset('hydrogen.n.01'), Synset('oxygen.n.01')] \n",
            "\n",
            "Entailment of word Snore:\n",
            " [Synset('sleep.v.01')] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snsm2e8cy7fb"
      },
      "source": [
        "TRIPLET EXTRACTION WITH EXAMPLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f77IjbLyZch"
      },
      "source": [
        "import re\r\n",
        "import pandas as pd\r\n",
        "import bs4\r\n",
        "import requests\r\n",
        "import spacy\r\n",
        "from spacy import displacy\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "from spacy.matcher import Matcher \r\n",
        "from spacy.tokens import Span \r\n",
        "\r\n",
        "import networkx as nx\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GT01OG5ydVy"
      },
      "source": [
        "candidate_sentences = \"the drawdown process is governed by astm standard d823\"\r\n",
        "doc = nlp(candidate_sentences)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzjViTjcyf5y",
        "outputId": "0f329ac1-0929-4826-9576-dbb1c8b77468"
      },
      "source": [
        "for tok in doc:\r\n",
        "    print(tok.text, \"...\", tok.dep_)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the ... det\n",
            "drawdown ... amod\n",
            "process ... nsubjpass\n",
            "is ... auxpass\n",
            "governed ... ROOT\n",
            "by ... agent\n",
            "astm ... compound\n",
            "standard ... amod\n",
            "d823 ... pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12wrUeJ9yj4b"
      },
      "source": [
        "def get_entities(sent):\r\n",
        "  ## chunk 1\r\n",
        "  ent1 = \"\"\r\n",
        "  ent2 = \"\"\r\n",
        "\r\n",
        "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\r\n",
        "  prv_tok_text = \"\"   # previous token in the sentence\r\n",
        "\r\n",
        "  prefix = \"\"\r\n",
        "  modifier = \"\"\r\n",
        "\r\n",
        "  #############################################################\r\n",
        "  \r\n",
        "  for tok in nlp(sent):\r\n",
        "    ## chunk 2\r\n",
        "    # if token is a punctuation mark then move on to the next token\r\n",
        "    if tok.dep_ != \"punct\":\r\n",
        "      # check: token is a compound word or not\r\n",
        "      if tok.dep_ == \"compound\":\r\n",
        "        prefix = tok.text\r\n",
        "        # if the previous word was also a 'compound' then add the current word to it\r\n",
        "        if prv_tok_dep == \"compound\":\r\n",
        "          prefix = prv_tok_text + \" \"+ tok.text\r\n",
        "      \r\n",
        "      # check: token is a modifier or not\r\n",
        "      if tok.dep_.endswith(\"mod\") == True:\r\n",
        "        modifier = tok.text\r\n",
        "        # if the previous word was also a 'compound' then add the current word to it\r\n",
        "        if prv_tok_dep == \"compound\":\r\n",
        "          modifier = prv_tok_text + \" \"+ tok.text\r\n",
        "      \r\n",
        "      ## chunk 3\r\n",
        "      if tok.dep_.find(\"subj\") == True:\r\n",
        "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\r\n",
        "        prefix = \"\"\r\n",
        "        modifier = \"\"\r\n",
        "        prv_tok_dep = \"\"\r\n",
        "        prv_tok_text = \"\"      \r\n",
        "\r\n",
        "      ## chunk 4\r\n",
        "      if tok.dep_.find(\"obj\") == True:\r\n",
        "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\r\n",
        "        \r\n",
        "      ## chunk 5  \r\n",
        "      # update variables\r\n",
        "      prv_tok_dep = tok.dep_\r\n",
        "      prv_tok_text = tok.text\r\n",
        "  #############################################################\r\n",
        "\r\n",
        "  return [ent1.strip(), ent2.strip()]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDWSrwgjynKB"
      },
      "source": [
        "def get_relation(sent):\r\n",
        "\r\n",
        "  doc = nlp(sent)\r\n",
        "\r\n",
        "  # Matcher class object \r\n",
        "  matcher = Matcher(nlp.vocab)\r\n",
        "\r\n",
        "  #define the pattern \r\n",
        "  pattern = [{'DEP':'ROOT'}, \r\n",
        "            {'DEP':'prep','OP':\"?\"},\r\n",
        "            {'DEP':'agent','OP':\"?\"},  \r\n",
        "            {'POS':'ADJ','OP':\"?\"}] \r\n",
        "\r\n",
        "  matcher.add(\"matching_1\", None, pattern) \r\n",
        "\r\n",
        "  matches = matcher(doc)\r\n",
        "  k = len(matches) - 1\r\n",
        "\r\n",
        "  span = doc[matches[k][1]:matches[k][2]] \r\n",
        "\r\n",
        "  return(span.text)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kuzz5m0GypmR",
        "outputId": "b392b0be-3a30-487f-edd6-3c09abc57b76"
      },
      "source": [
        "get_relation(\"John completed the task\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'completed'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ali88aJnysWR"
      },
      "source": [
        "text=\"John completed the task\""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjCbfhcnyuUk"
      },
      "source": [
        "ent=get_entities(text)\r\n",
        "rel=get_relation(text)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zp40V4cywID",
        "outputId": "81e8a255-e544-4755-a0de-27e877113c32"
      },
      "source": [
        "ent"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drawdown  process', 'astm standard']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGKh76IUyy-B",
        "outputId": "7c377c28-82f0-4d7a-8d0f-d1f52828431b"
      },
      "source": [
        "new_list=[]\r\n",
        "if len(ent)==2:\r\n",
        "    for i,n in enumerate(ent):\r\n",
        "        #print(i,n)\r\n",
        "        if i==1:\r\n",
        "            new_list.append(rel) \r\n",
        "        else:\r\n",
        "            new_list.append(n)\r\n",
        "    new_list.append(ent[1])\r\n",
        "print(new_list)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['drawdown  process', 'governed by', 'astm standard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxQPr384y3wJ",
        "outputId": "e8f2a172-3e77-46e1-bc75-bcd530f6bb56"
      },
      "source": [
        "text=\"the drawdown process is governed by astm standard\"\r\n",
        "ent=get_entities(text)\r\n",
        "rel=get_relation(text)\r\n",
        "new_list=[]\r\n",
        "if len(ent)==2:\r\n",
        "    for i,n in enumerate(ent):\r\n",
        "        #print(i,n)\r\n",
        "        if i==1:\r\n",
        "            new_list.append(rel) \r\n",
        "        else:\r\n",
        "            new_list.append(n)\r\n",
        "    new_list.append(ent[1])\r\n",
        "print(new_list)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['drawdown  process', 'governed by', 'astm standard']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}